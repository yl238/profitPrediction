import pandas as pd

def calc_lift(x,y,clf,bins=10):
    """
    Takes input arrays and trained SkLearn Classifier and returns a Pandas
    DataFrame with the average lift generated by the model in each bin
    
    Parameters
    -------------------
    x:    Numpy array or Pandas Dataframe with shape = [n_samples, n_features]

    y:    A 1-d Numpy array or Pandas Series with shape = [n_samples]
         IMPORTANT: Code is only configured for binary target variable
         of 1 for success and 0 for failure

    clf:  A trained SkLearn classifier object
    bins: Number of equal sized buckets to divide observations across
      Default value is 10
    """

    #Actual Value of y
    y_actual = y
    #Predicted Probability that y = 1
    y_prob = clf.predict_proba(x)[:,1]
    #Predicted Value of Y
    y_pred = clf.predict(x)
    cols = ['actual','proba_positive','predicted']
    data = [y_actual,y_prob, y_pred]
    df = pd.DataFrame(dict(zip(cols,data)))
    
    #Observations where y=1
    total_positive_n = df['actual'].sum()
    #Total Observations
    total_n = df.index.size
    natural_positive_prob = total_positive_n/float(total_n)


    #Create Bins where First Bin has Observations with the
    #Highest Predicted Probability that y = 1
    df['bin_positive'] = pd.qcut(df['proba_positive'],bins,labels=False)
    
    pos_group_df = df.groupby('bin_positive')
    #Percentage of Observations in each Bin where y = 1 
    lift_positive = pos_group_df['actual'].sum()/pos_group_df['actual'].count()
    lift_index_positive = list((lift_positive/natural_positive_prob)*100)
    lift_positive = list(lift_positive)
    
    lift_positive.append(0)
    lift_index_positive.append(0)
    natural_positive_prob_list = [natural_positive_prob for i in range(len(lift_positive))]
    natural_positive_prob_list[-1] = 0
    
    
    #Consolidate Results into Output Dataframe
    lift_df = pd.DataFrame({'lift_positive':lift_positive[::-1],
                               'lift_positive_index':lift_index_positive[::-1],
                               'baseline_positive':natural_positive_prob_list[::-1]})
    
    return lift_df

def plot_feature_importances(features_df, model, limit=10):
    """
    Plot feature importance for a given model (trees & ensembles)
    """
    n_features = len(features_df.columns.values)
    
    feature_importance = pd.DataFrame()
    feature_importance['importance'] = model.feature_importances_
    feature_importance['feature_names'] = features_df.columns.values
    # sort in terms of magnitude
    feature_importance.sort_values(by='importance',inplace=True)
    
    plt.barh(range(limit), feature_importance['importance'].values[-limit:], align='center')
    plt.yticks(np.arange(limit), feature_importance['feature_names'].values[-limit:])
    plt.xlabel("Feature Importance")
    plt.ylabel("Feature")
    
    